<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>pytorch学习 | 大鸟来啦✈️~</title><meta name="keywords" content="大鸟,博客"><meta name="author" content="本大鸟就是玩✈️~"><meta name="copyright" content="本大鸟就是玩✈️~"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="安装环境 参考视频 使用 Anaconda Navigator 换源（最简单） 因为学完了吴恩达机器学习，然后数学公式这边有点欠缺，打算看大部分西瓜书和统计学习方法后再学那个白板；pytorch也是在anaconda中安装环境，conda create -n pytorch python&#x3D;3.6创建名为pytorch的环境，激活环境conda activate pytorch在进入环境安装pyto"><meta property="og:type" content="article"><meta property="og:title" content="pytorch学习"><meta property="og:url" content="https://www.hgez6.top/posts/b118e8cc/index.html"><meta property="og:site_name" content="大鸟来啦✈️~"><meta property="og:description" content="安装环境 参考视频 使用 Anaconda Navigator 换源（最简单） 因为学完了吴恩达机器学习，然后数学公式这边有点欠缺，打算看大部分西瓜书和统计学习方法后再学那个白板；pytorch也是在anaconda中安装环境，conda create -n pytorch python&#x3D;3.6创建名为pytorch的环境，激活环境conda activate pytorch在进入环境安装pyto"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://yun.hgez6.top/api/raw/?path=/picbed/12.jpg"><meta property="article:published_time" content="2023-11-21T13:16:41.000Z"><meta property="article:modified_time" content="2023-12-28T10:54:10.703Z"><meta property="article:author" content="本大鸟就是玩✈️~"><meta property="article:tag" content="大鸟,博客"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://yun.hgez6.top/api/raw/?path=/picbed/12.jpg"><link rel="shortcut icon" href="/"><link rel="canonical" href="https://www.hgez6.top/posts/b118e8cc/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="baidu-site-verification" content="code-pgxsym62GU"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.min.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?530e82ef2951b6eb3701161fed246350";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!0,languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:{limitDay:365,position:"top",messagePrev:"It has been",messageNext:"距离上一次更新文章已经太久请酌情阅读"},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:230},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"/pluginsSrc/flickr-justified-gallery/dist/fjGallery.min.js",css:"/pluginsSrc/flickr-justified-gallery/dist/fjGallery.css"}},isPhotoFigcaption:!0,islazyload:!0,isAnchor:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"pytorch学习",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2023-12-28 18:54:10"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,n={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(n))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise(((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)})),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=18?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside")),/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><link rel="stylesheet" href="/css/diycustom.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="blurNum"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><canvas id="bubble"></canvas><link rel="stylesheet" href="/css/diybolang.css"><script src="/js/SAO-Notify.js" async></script><svg aria-hidden="true" style="position:absolute;overflow:hidden;width:0;height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248 626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><link rel="stylesheet" href="/css/chajian/swiper.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/css/chajian/swiperstyle.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/css/chajian/animate.min.css" media="print" onload='this.media="screen"'><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="/css/chajian/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="/css/chajian/tag_plugins.css" media="defer" onload='this.media="all"'><script src="/js/chajian/carousel-touch.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="大鸟来啦✈️~" type="application/atom+xml"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://yun.hgez6.top/api/raw/?path=/picbed/tx.png" onerror='onerror=null,src="/assets/r1.jpg"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">122</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">52</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">34</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--article"></use></svg> <span class="menu_word" style="font-size:17px">文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">时间轴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:randomPost();"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-suiji"></use></svg> <span class="menu_word" style="font-size:17px">随便看看</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pinweishenghuo"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/Bmovies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">视频</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/mypicture/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangce"></use></svg> <span class="menu_word" style="font-size:17px">图库</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-niao"></use></svg> <span class="menu_word" style="font-size:17px">日常</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying"></use></svg> <span class="menu_word" style="font-size:17px">观影</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/books/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shu"></use></svg> <span class="menu_word" style="font-size:17px">看书</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-youxi"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangzi"></use></svg> <span class="menu_word" style="font-size:17px">八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/zm/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yun"></use></svg> <span class="menu_word" style="font-size:17px">云电脑</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/guidehg/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-jingzi"></use></svg> <span class="menu_word" style="font-size:17px">引导页</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://yun.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zuzhiheguanlitubiao-"></use></svg> <span class="menu_word" style="font-size:17px">云空间</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://e5.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wuxian"></use></svg> <span class="menu_word" style="font-size:17px">E5无限云</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/zjapi/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-APIjiekou"></use></svg> <span class="menu_word" style="font-size:17px">自建API</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://gd.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qiehuan"></use></svg> <span class="menu_word" style="font-size:17px">切换主题</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shejiaoxinxi"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wangye"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://status.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-jiancefanwei"></use></svg> <span class="menu_word" style="font-size:17px">状态监测</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/charts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/aboutweb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guanyuwomen"></use></svg> <span class="menu_word" style="font-size:17px">关于本站</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-love-sign"></use></svg> <span class="menu_word" style="font-size:17px">恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">大鸟来啦✈️~</a></span><div id="he-plugin-simple"></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-home"></use></svg> <span class="menu_word" style="font-size:17px">首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--article"></use></svg> <span class="menu_word" style="font-size:17px">文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guidang1"></use></svg> <span class="menu_word" style="font-size:17px">时间轴</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-sekuaibiaoqian"></use></svg> <span class="menu_word" style="font-size:17px">标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-fenlei"></use></svg> <span class="menu_word" style="font-size:17px">分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:randomPost();"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-suiji"></use></svg> <span class="menu_word" style="font-size:17px">随便看看</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pinweishenghuo"></use></svg> <span class="menu_word" style="font-size:17px">休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yinle"></use></svg> <span class="menu_word" style="font-size:17px">音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/Bmovies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying1"></use></svg> <span class="menu_word" style="font-size:17px">视频</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/mypicture/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangce"></use></svg> <span class="menu_word" style="font-size:17px">图库</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-niao"></use></svg> <span class="menu_word" style="font-size:17px">日常</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-dianying"></use></svg> <span class="menu_word" style="font-size:17px">观影</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/books/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shu"></use></svg> <span class="menu_word" style="font-size:17px">看书</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-youxi"></use></svg> <span class="menu_word" style="font-size:17px">游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-xiangzi"></use></svg> <span class="menu_word" style="font-size:17px">八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/zm/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-yun"></use></svg> <span class="menu_word" style="font-size:17px">云电脑</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/guidehg/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-jingzi"></use></svg> <span class="menu_word" style="font-size:17px">引导页</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://yun.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zuzhiheguanlitubiao-"></use></svg> <span class="menu_word" style="font-size:17px">云空间</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://e5.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wuxian"></use></svg> <span class="menu_word" style="font-size:17px">E5无限云</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/zjapi/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-APIjiekou"></use></svg> <span class="menu_word" style="font-size:17px">自建API</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://gd.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qiehuan"></use></svg> <span class="menu_word" style="font-size:17px">切换主题</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-zhifengche"></use></svg> <span class="menu_word" style="font-size:17px">网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shejiaoxinxi"></use></svg> <span class="menu_word" style="font-size:17px">社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-pengyouquan"></use></svg> <span class="menu_word" style="font-size:17px">朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-liuyan"></use></svg> <span class="menu_word" style="font-size:17px">留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-lianjie"></use></svg> <span class="menu_word" style="font-size:17px">友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-wangye"></use></svg> <span class="menu_word" style="font-size:17px">网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://status.hgez6.top/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-jiancefanwei"></use></svg> <span class="menu_word" style="font-size:17px">状态监测</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon--tongjibiao"></use></svg> <span class="menu_word" style="font-size:17px">网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/charts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-shujutongji1"></use></svg> <span class="menu_word" style="font-size:17px">文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/aboutweb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-guanyuwomen"></use></svg> <span class="menu_word" style="font-size:17px">关于本站</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-maoliang"></use></svg> <span class="menu_word" style="font-size:17px">个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-qunliaotian"></use></svg> <span class="menu_word" style="font-size:17px">唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-love-sign"></use></svg> <span class="menu_word" style="font-size:17px">恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.3em;height:1.3em;vertical-align:-.15em;fill:currentColor;overflow:hidden"><use xlink:href="#icon-paperplane"></use></svg> <span class="menu_word" style="font-size:17px">关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg> <span>搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-tupian1"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">pytorch学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于</span> <time class="post-meta-date-created" datetime="2023-11-21T13:16:41.000Z" title="发表于 2023-11-21 21:16:41">2023-11-21</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-28T10:54:10.703Z" title="更新于 2023-12-28 18:54:10">2023-12-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">4754</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="pytorch学习"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s58 18 88 18 58-18 88-18 58 18 88 18v44h-352Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="安装环境">安装环境</h1><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/?p=2&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=ea78e03d02c651ab9a1849ee5454e98b">参考视频</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43198568/article/details/105626024">使用 Anaconda Navigator 换源（最简单）</a><br>因为学完了吴恩达机器学习，然后数学公式这边有点欠缺，打算看大部分西瓜书和统计学习方法后再学那个白板；pytorch也是在anaconda中安装环境，<code>conda create -n pytorch python=3.6</code>创建名为pytorch的环境，激活环境<code>conda activate pytorch</code>在进入环境安装pytorch，注意cuda版本是可以选的，用<code>nvidia-smi</code>查看当前显卡驱动的cuda版本也就是这个版本可以自定义，那么上次sleap出错会不会是cuda版本的问题，然后在<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">pytorch官网</a>选择合适的版本生成安装代码例如<code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia</code></p><h1 id="使用环境">使用环境</h1><p>pycharm可以选择anaconda中创建的环境</p><h1 id="巧用dir-help-pyc终端">巧用dir,help,pyc终端</h1><p>dir()打开，看见<br>help():说明书<br>dir(pytorch)<br>输出：1、2、3<br>dir(pytorch.3)<br>输出：<br>a.b,c<br>help(pytorch.3.a)<br>输出：<br>将此扳手放在特定地方，然后拧动<br>看到dir输出__a__说明具体到函数了<br>pyc终端中可以试验函数的具体数值如b=range(1,3)</p><h2 id="pyc切换解释器">pyc切换解释器</h2><p>在编辑配置里添加python解释器选择对应的解释器,pyc里可以右键文件选择复制全局/相对路径，右下角也可以直接切换解释器</p><h1 id="pytorch加载数据初认识">PyTorch加载数据初认识</h1><p>Dataset：提供一种方式去获取数据及其label；如何获取每一个数据及其label；告诉我们总共有多少的数据<br>Dataloader：为后面的网络提供不同的数据形式<br>P6加载数据的类代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,root_dir,label_dir</span>):</span><br><span class="line">        self.root_dir=root_dir</span><br><span class="line">        self.label_dir=label_dir</span><br><span class="line">        self.path=os.path.join(self.root_dir,self.label_dir)</span><br><span class="line">        self.img_path=os.listdir(self.path)</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">            img_name=self.img_path[idx]</span><br><span class="line">            img_item_path=os.path.join(self.root_dir,self.label_dir,img_name)</span><br><span class="line">            img=Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">            label=self.label_dir</span><br><span class="line">            <span class="keyword">return</span> img,labeld</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br></pre></td></tr></table></figure><p>对蚂蚁图像批量赋予标签代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">root_dir=<span class="string">&quot;练手数据集/train&quot;</span></span><br><span class="line">target_dir=<span class="string">&quot;ants_image&quot;</span></span><br><span class="line">img_path=os.listdir(os.path.join(root_dir,target_dir))</span><br><span class="line"><span class="built_in">print</span>(img_path)</span><br><span class="line"><span class="comment"># import pdb</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># pdb.set_trace()</span></span><br><span class="line"><span class="comment"># 比如ants_label就是取前面的ants</span></span><br><span class="line">label=target_dir.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">out_dir=<span class="string">&quot;ants_label&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> img_path:</span><br><span class="line">    file_name=i.split(<span class="string">&#x27;.jpg&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#输出至目录并写入label如ants</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(root_dir,out_dir,<span class="string">&quot;&#123;&#125;.txt&quot;</span>.<span class="built_in">format</span>(file_name)),<span class="string">&#x27;w&#x27;</span>)<span class="keyword">as</span> f:</span><br><span class="line">        f.write(label)</span><br></pre></td></tr></table></figure><p>如果报错把<code>root_dir=&quot;练手数据集/train&quot;</code>改成<code>root_dir=./&quot;练手数据集/train&quot;</code>即可</p><h1 id="p9">P9</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># python的用法-》tensor数据类型</span></span><br><span class="line"><span class="comment"># 通过transforms.ToTensor:去看两个问测</span></span><br><span class="line"><span class="comment">#2、为什么我们需要Tensor数据类型</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line">img_path=<span class="string">&quot;./练手数据集/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line">img=Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="comment">#1、transforms该如何使用(python)</span></span><br><span class="line"><span class="comment"># 调用库中的类工具</span></span><br><span class="line">tensor_trans=transforms.ToTensor()</span><br><span class="line"><span class="comment"># 类的实例化然后传值？</span></span><br><span class="line">tensor_img=tensor_trans(img)</span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>,tensor_img)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="p10">P10</h1><p>pycharm终端无法显示环境,调用log得手动打代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate pytorch</span><br><span class="line">tensorboard --logdir=logs</span><br></pre></td></tr></table></figure><h3 id="常见的transforms">常见的Transforms</h3><p>*输入 *PiL *Image.open()<br>*输出 *tensor *ToTensor()<br>*作用 *narrays *cv.imread()</p><h3 id="类中-call-的用法">类中__call__的用法</h3><p>使用call可以直接给实例赋值调用。不需要子函数，说白了就是c++的运算符重载？</p><h3 id="pyc查看结构">pyc查看结构</h3><p>ctrl+左键进入模块py目录，左侧查看模块结构和函数，ctrl+P查看属性提示<br>tensor后的图片tensor_img[2][1][0],第一个2似乎表示通道，第二，3个表征像素位置，从0开始计数<br>writer.add_image(&quot;Tensor_imgN&quot;,img_norm,2)最后一个参数可调整步长，滑动查看之前的log</p><h3 id="randomcrop-随机裁剪但也能指定参数">RandomCrop 随机裁剪但也能指定参数</h3><p>指定参数的作用是随机裁剪的大小，必须小于原图<br>compose就相当于前面的两步转为一步,图片操作的组合</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># python的用法-》tensor数据类型</span></span><br><span class="line"><span class="comment"># 通过transforms.ToTensor:去看两个问测</span></span><br><span class="line"><span class="comment">#2、为什么我们需要Tensor数据类型</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line">img_path=<span class="string">&quot;./练手数据集/train/ants_image/0013035.jpg&quot;</span></span><br><span class="line">img=Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ToTensor()</span></span><br><span class="line"><span class="string">的使用</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#1、transforms该如何使用(python)</span></span><br><span class="line"><span class="comment"># 调用库中的类工具</span></span><br><span class="line">tensor_trans=transforms.ToTensor()</span><br><span class="line"><span class="comment"># 类的实例化然后传值？</span></span><br><span class="line">tensor_img=tensor_trans(img)</span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_img&quot;</span>,tensor_img)</span><br><span class="line"><span class="comment"># writer.close()</span></span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">output[channel] = </span></span><br><span class="line"><span class="string">(input[channel] - mean[channel]) / std[channel]</span></span><br><span class="line"><span class="string">两个长度为三的数组，</span></span><br><span class="line"><span class="string">分别表示三个通道的平均值和标准差</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 第一个0似乎表示通道，第二，3个表征像素位置，从0开始计数</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor_img[<span class="number">2</span>][<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">trans_norm=transforms.Normalize([<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">0.5</span>],[<span class="number">1</span>,<span class="number">0.5</span>,<span class="number">0.5</span>])</span><br><span class="line">img_norm=trans_norm(tensor_img)</span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">2</span>][<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">writer.add_image(<span class="string">&quot;Tensor_imgN&quot;</span>,img_norm,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Resize</span></span><br><span class="line"><span class="built_in">print</span>(img.size)</span><br><span class="line">trans_resize=transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line"><span class="comment">#img PIL -&gt;resize -&gt;img_resize PIL</span></span><br><span class="line">img_resize=trans_resize(img)</span><br><span class="line"><span class="comment">#img_resize PIL -&gt;totensor -&gt;img_resize tensor</span></span><br><span class="line">img_resize=tensor_trans(img_resize)</span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>,img_resize,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#Compose resize 2</span></span><br><span class="line">trans_resize_2=transforms.Resize(<span class="number">512</span>)</span><br><span class="line"><span class="comment"># compose就相当于前面的两步转为一步</span></span><br><span class="line">trans_compose=transforms.Compose([trans_resize_2,tensor_trans])</span><br><span class="line">img_resize_2=trans_compose(img)</span><br><span class="line">writer.add_image(<span class="string">&quot;Resize&quot;</span>,img_resize_2,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># RandomCrop 随机裁剪但也能指定参数</span></span><br><span class="line">trans_random=transforms.RandomCrop(<span class="number">51</span>)</span><br><span class="line">trans_compose_2=transforms.Compose([trans_random,tensor_trans])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img_crop=trans_compose_2(img)</span><br><span class="line">    writer.add_image(<span class="string">&quot;Randomcrop&quot;</span>,img_crop,i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="p15">P15</h1><p>batch_size的作用是在test_data中取若干个数据</p><h1 id="p16-神经网络的基本骨架-nn-module的使用">P16 神经网络的基本骨架-nn.Module的使用</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">    x=F.relu(self.conv1(x))</span><br><span class="line">    <span class="keyword">return</span> F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure><p>以上代码进行了如下操作：<br>1输入：X<br>2卷积<br>3非线性<br>4卷积<br>5非线性<br>6输出<br>在pyc中Windows ctrl+o可以重写方法</p><h1 id="p16">P16</h1><p>torch.tensor和torch.Tensor也有区别哦</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">                       ,[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">                       ,[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">kernel=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment">#尺寸变换以适应Input四参数 经查证红字有误 第一个参数是batch size样本数量 第二个参数是channel图像的通道数量</span></span><br><span class="line"><span class="built_in">input</span>=torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel=torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"><span class="built_in">print</span>(kernel.shape)</span><br><span class="line">output=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h1 id="p18">P18</h1><p>Out_channel=3相当于三个卷积核卷积后叠加输出的数组，其形式相当于RGB三通道<br>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span>  torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">BATCH_SIZE的含义</span></span><br><span class="line"><span class="string">BATCH_SIZE:即一次训练所抓取的数据样本数量；</span></span><br><span class="line"><span class="string">BATCH_SIZE的大小影响训练速度和模型优化。同时按照以上代码可知，其大小同样影响每一epoch训练模型次数。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        self.conv1=Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#forward写卷积逻辑跟函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">step=<span class="number">0</span></span><br><span class="line">tudui1=Tudui()</span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tudui1)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    output=tudui1(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    output=torch.reshape(output,(-<span class="number">1</span>,<span class="number">3</span>,<span class="number">30</span>,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br></pre></td></tr></table></figure><h1 id="p19">P19</h1><p>池化层的步长默认是卷积核的大小,最大池化，就是取其中的最大值<br>ceil 允许有出界部分；floor 不允许<br>池化和卷积不一样的<br>矩阵版代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">[<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">[<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="comment"># 原来是个二维的现在成四维的了</span></span><br><span class="line"><span class="built_in">input</span>=torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        self.maxpool1=MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output=self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span>  output</span><br><span class="line">tudui=Tudui()</span><br><span class="line">output=tudui(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p>数据集版代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        self.maxpool1=MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output=self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span>  output</span><br><span class="line">tudui=Tudui()</span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;dataloadermaxpoolc&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets=data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    output=tudui(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step=step+<span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>数据集版导入数据集后处理后的照片像是加了一层马赛克</p><h1 id="p20">P20</h1><p>nn_relu.py<br>矩阵版代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,-<span class="number">0.5</span>],</span><br><span class="line">[-<span class="number">1</span>,<span class="number">3</span>]])</span><br><span class="line">output=torch.reshape(<span class="built_in">input</span>,(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        self.relu1=ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output=self.relu1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">tudui=Tudui()</span><br><span class="line">output=tudui(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h1 id="p21">P21</h1><p>报错是因为数据集batch之后剩了16张图片，dataloader的参数drop_last设为true就不报错了,torch.flatten()用于把数据展平比如把二维展平成一维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader=DataLoader(dataset, batch_size=<span class="number">64</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        self.linear1=Linear(<span class="number">196608</span>,<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output=self.linear1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tudui=Tudui()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets=data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># 变换维度以传入神经网络</span></span><br><span class="line">    output=torch.reshape(imgs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    output=torch.flatten(imgs)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    output=tudui(output)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h1 id="p22">P22</h1><p>CIFAR10代表有十个类别,<a target="_blank" rel="noopener" href="https://blog.csdn.net/hy592070616/article/details/129504980">torch.ones</a><br>这个是不太推荐的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        <span class="comment">#详情看图</span></span><br><span class="line">        self.conv1=Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool1=MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2=Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool2=MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3=Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool3=MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.flatten=Flatten()</span><br><span class="line">        self.linear1=Linear(<span class="number">1024</span>,<span class="number">64</span>)</span><br><span class="line">        self.linear2=Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.conv1(x)</span><br><span class="line">        x=self.maxpool1(x)</span><br><span class="line">        x=self.conv2(x)</span><br><span class="line">        x=self.maxpool2(x)</span><br><span class="line">        x=self.conv3(x)</span><br><span class="line">        x=self.maxpool3(x)</span><br><span class="line">        x=self.flatten(x)</span><br><span class="line">        x=self.linear1(x)</span><br><span class="line">        x=self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">tudui=Tudui()</span><br><span class="line"><span class="built_in">print</span>(tudui)</span><br><span class="line"><span class="built_in">input</span>=torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">output=tudui(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p>网络整合的版本</p><h1 id="p23">P23</h1><p>所以是反向传播来计算梯度，根据梯度来更新参数，实现loss最小化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line">inputs=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line">inputs=torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets=torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss=L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">result=loss(inputs,targets)</span><br><span class="line">loss_mse=nn.MSELoss()</span><br><span class="line">result_mse=loss_mse(inputs,targets)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br><span class="line">x=torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])</span><br><span class="line">y=torch.tensor([<span class="number">1</span>])</span><br><span class="line">x=torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss_cross=nn.CrossEntropyLoss()</span><br><span class="line">result_cross=loss_cross(x,y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure><h1 id="p24">P24</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader=DataLoader(dataset, batch_size=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        <span class="comment">#详情看图</span></span><br><span class="line">        self.model1=Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">tudui=Tudui()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets=data</span><br><span class="line">    outputs=tudui(imgs)</span><br><span class="line">    <span class="built_in">print</span>(outputs)</span><br><span class="line">    <span class="built_in">print</span>(targets)</span><br><span class="line">    loss=nn.CrossEntropyLoss()</span><br><span class="line">    result_loss=loss(outputs, targets)</span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br><span class="line">    result_loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ok&quot;</span>)</span><br></pre></td></tr></table></figure><h1 id="p25">P25</h1><p>优化问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader=DataLoader(dataset, batch_size=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui,self).__init__()</span><br><span class="line">        <span class="comment">#详情看图</span></span><br><span class="line">        self.model1=Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">tudui=Tudui()</span><br><span class="line">optim=torch.optim.SGD(tudui.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs,targets=data</span><br><span class="line">        outputs=tudui(imgs)</span><br><span class="line">        loss=nn.CrossEntropyLoss()</span><br><span class="line">        result_loss=loss(outputs, targets)</span><br><span class="line">        <span class="comment">#梯度清理防止保留上次的梯度迭代</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        result_loss.backward()</span><br><span class="line">        optim.step()</span><br><span class="line">        running_loss=running_loss+result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure><h1 id="p26">P26</h1><p>pretrained=True就是会加载网络默认的参数<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41278720/article/details/80759933">参考</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_data=torchvision.datasets.ImageNet(&quot;../data_image_net&quot;,split=&quot;train&quot;,download=True,transform=torchvision.transforms.ToTensor())</span></span><br><span class="line">vgg16_true=torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg16_false=torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line"></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 对vgg16_true的classifier层增加一层线性层</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&quot;add_linear&quot;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br><span class="line"><span class="comment"># vgg16的[6]层进行修改,线性层修改为4096，10</span></span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>]=nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure><h1 id="p27">P27</h1><p>保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16=torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 保存方式1保存网络结构+模型参数</span></span><br><span class="line">torch.save(vgg16,<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="comment">#加载模型方式1</span></span><br><span class="line">model=torch.load(<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 保存方式2，模型参数（官方推荐空间小）</span></span><br><span class="line">torch.save(vgg16.state_dict(),<span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br><span class="line"><span class="comment">#加载模型参数</span></span><br><span class="line">model=torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()</span><br><span class="line">        self.Conv1=nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x=self.Conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">tudui=Tudui()</span><br><span class="line">torch.save(tudui,<span class="string">&quot;tudui_method1.pth&quot;</span>)</span><br><span class="line"><span class="comment">#这句话如果要在别的py文件里加载是要复制Class类或者import Tudui</span></span><br><span class="line">model1=torch.load(<span class="string">&quot;tudui_method1.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model1)</span><br></pre></td></tr></table></figure><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/?p=27&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=ea78e03d02c651ab9a1849ee5454e98b">CNN中stride（步幅）和padding（填充）的详细理解</a><br>图像卷积后像素大小没变</p><h1 id="p28">P28</h1><p>item()可以把tensor中的数值取出来,应该说0是指定第0轴[竖着看]，1指定第1轴[横着看]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Tudui</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;logs_train&quot;</span>)</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line">test_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size=<span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size=<span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size代表训练数据集的长度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader来加载数据集</span></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader=DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"><span class="comment">#创建网络模型</span></span><br><span class="line">tudui=Tudui()</span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn=nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line">learning_rate=<span class="number">1e-2</span></span><br><span class="line">optimizer=torch.optim.SGD(tudui.parameters(),lr=learning_rate)</span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line">total_train_step=<span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch=<span class="number">10</span></span><br><span class="line">total_test_step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;轮训练开始&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets=data</span><br><span class="line">        outputs=tudui(imgs)</span><br><span class="line">        loss=loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器模型cc</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step=total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数:&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    total_test_loss=<span class="number">0</span></span><br><span class="line">    total_accuracy=<span class="number">0</span></span><br><span class="line">    <span class="comment"># with torch.no_grad():的意思是测试过程中不用对模型进行调优直接用现有的模型就行</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets=data</span><br><span class="line">            outputs=tudui(imgs)</span><br><span class="line">            loss=loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss=total_test_loss+loss.item()</span><br><span class="line">            accuracy=(outputs.argmax(<span class="number">1</span>)==targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy=total_accuracy+accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的LOSS：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, loss.item(), total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    torch.save(tudui,<span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="gpu训练">GPU训练</h1><p>第一种训练方式新增以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tudui=tudui.cuda()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn=loss_fn.cuda()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    imgs=imgs.cuda()</span><br><span class="line">    targets=targets.cuda()</span><br></pre></td></tr></table></figure><p>start_time=time.time()<br>end_time = time.time()<br>这两句话可以看情况加<br>tudui.train()<br>tudui.eval()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Tudui</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;logs_train&quot;</span>)</span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line">test_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size=<span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size=<span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size代表训练数据集的长度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader来加载数据集</span></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader=DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"><span class="comment">#创建网络模型</span></span><br><span class="line">tudui=Tudui()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tudui=tudui.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn=nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn=loss_fn.cuda()</span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line">learning_rate=<span class="number">1e-2</span></span><br><span class="line">optimizer=torch.optim.SGD(tudui.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line">total_train_step=<span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch=<span class="number">10</span></span><br><span class="line">total_test_step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;轮训练开始&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets=data</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            imgs=imgs.cuda()</span><br><span class="line">            targets=targets.cuda()</span><br><span class="line">        outputs=tudui(imgs)</span><br><span class="line">        loss=loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器模型cc</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step=total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(end_time - start_time)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数:&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss=<span class="number">0</span></span><br><span class="line">    total_accuracy=<span class="number">0</span></span><br><span class="line">    <span class="comment"># with torch.no_grad():的意思是测试过程中不用对模型进行调优直接用现有的模型就行</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets=data</span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line">            outputs=tudui(imgs)</span><br><span class="line">            loss=loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss=total_test_loss+loss.item()</span><br><span class="line">            accuracy=(outputs.argmax(<span class="number">1</span>)==targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy=total_accuracy+accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的LOSS：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, loss.item(), total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    torch.save(tudui,<span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>第二种训练方式主要使用语句<code>device=torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code><br>在第9行要加image=image.convert('RGB')<br>因为png格式是四个通道，除了RGB三通道外，还有一个透明度通道。<br>所以，我们调用image=image.convert(RGB),保留其颜色通道<br>当然，如果图片本来就是三个颜色通道，经过此操作，不变。<br>加上这一步后，可以适应png,jpg各种格式的图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Tudui</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义训练设备</span></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># device=torch.device(&quot;cuda&quot;)</span></span><br><span class="line"><span class="comment"># device=torch.device(&quot;cpu&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;logs_train&quot;</span>)</span><br><span class="line">start_time=time.time()</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line">test_data=torchvision.datasets.CIFAR10(root=<span class="string">&quot;../data&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),</span><br><span class="line">download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size=<span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size=<span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size代表训练数据集的长度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用DataLoader来加载数据集</span></span><br><span class="line">train_dataloader=DataLoader(train_data,batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader=DataLoader(test_data,batch_size=<span class="number">64</span>)</span><br><span class="line"><span class="comment">#创建网络模型</span></span><br><span class="line">tudui=Tudui()</span><br><span class="line">tudui=tudui.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss_fn=nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#这个不另外赋值</span></span><br><span class="line">loss_fn=loss_fn.to(device)</span><br><span class="line"><span class="comment">#优化器</span></span><br><span class="line">learning_rate=<span class="number">1e-2</span></span><br><span class="line">optimizer=torch.optim.SGD(tudui.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line">total_train_step=<span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮数</span></span><br><span class="line">epoch=<span class="number">10</span></span><br><span class="line">total_test_step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;第&#123;&#125;轮训练开始&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs,targets=data</span><br><span class="line">        imgs=imgs.to(device)</span><br><span class="line">        targets=targets.to(device)</span><br><span class="line">        outputs=tudui(imgs)</span><br><span class="line">        loss=loss_fn(outputs,targets)</span><br><span class="line">        <span class="comment"># 优化器模型cc</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step=total_train_step+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(end_time - start_time)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数:&#123;&#125;，Loss:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss=<span class="number">0</span></span><br><span class="line">    total_accuracy=<span class="number">0</span></span><br><span class="line">    <span class="comment"># with torch.no_grad():的意思是测试过程中不用对模型进行调优直接用现有的模型就行</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs,targets=data</span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                imgs = imgs.cuda()</span><br><span class="line">                targets = targets.cuda()</span><br><span class="line">            outputs=tudui(imgs)</span><br><span class="line">            loss=loss_fn(outputs,targets)</span><br><span class="line">            total_test_loss=total_test_loss+loss.item()</span><br><span class="line">            accuracy=(outputs.argmax(<span class="number">1</span>)==targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy=total_accuracy+accuracy</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的LOSS：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    total_test_step = total_test_step+<span class="number">1</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, loss.item(), total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_size, total_test_step)</span><br><span class="line">    torch.save(tudui,<span class="string">&quot;tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">model=torch.load(<span class="string">&quot;tudui_0.pth&quot;</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>输入/输出类型都应该为cuda，如果出现常见错误: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor)，则添加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device=torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)  </span><br><span class="line">image=image.to(device)  </span><br></pre></td></tr></table></figure><p>如果模型参数使用gpu训练的而本机环境只有CPU则用语句<code>map_location=torch.device('cpu')</code>转换</p></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>pytorch学习</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.hgez6.top/posts/b118e8cc/">https://www.hgez6.top/posts/b118e8cc/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>本大鸟就是玩✈️~</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-11-21</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2023-12-28</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="business-center"><div class="business-card"><div class="business-flip"><div class="business-front"><div class="business-strip-bottom"></div><div class="business-strip-top"></div><img class="business-logo" width="80" height="80" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/50/v2-debb46ef731baa386482f9cddc7cbc21_720w.webp?source=1940ef5c" alt="cardcode"><div class="business-Invitation">Invitation</div><div class="business-chip"><div class="business-chip-line"></div><div class="business-chip-line"></div><div class="business-chip-line"></div><div class="business-chip-line"></div><div class="business-chip-main"></div></div><svg class="business-wave" viewBox="0 3.71 26.959 38.787" width="26.959" height="38.787" fill="white"><path d="M19.709 3.719c.266.043.5.187.656.406 4.125 5.207 6.594 11.781 6.594 18.938 0 7.156-2.469 13.73-6.594 18.937-.195.336-.57.531-.957.492a.9946.9946 0 0 1-.851-.66c-.129-.367-.035-.777.246-1.051 3.855-4.867 6.156-11.023 6.156-17.718 0-6.696-2.301-12.852-6.156-17.719-.262-.317-.301-.762-.102-1.121.204-.36.602-.559 1.008-.504z"></path><path d="M13.74 7.563c.231.039.442.164.594.343 3.508 4.059 5.625 9.371 5.625 15.157 0 5.785-2.113 11.097-5.625 15.156-.363.422-1 .472-1.422.109-.422-.363-.472-1-.109-1.422 3.211-3.711 5.156-8.551 5.156-13.843 0-5.293-1.949-10.133-5.156-13.844-.27-.309-.324-.75-.141-1.114.188-.367.578-.582.985-.542h.093z"></path><path d="M7.584 11.438c.227.031.438.144.594.312 2.953 2.863 4.781 6.875 4.781 11.313 0 4.433-1.828 8.449-4.781 11.312-.398.387-1.035.383-1.422-.016-.387-.398-.383-1.035.016-1.421 2.582-2.504 4.187-5.993 4.187-9.875 0-3.883-1.605-7.372-4.187-9.875-.321-.282-.426-.739-.266-1.133.164-.395.559-.641.984-.617h.094zM1.178 15.531c.121.02.238.063.344.125 2.633 1.414 4.437 4.215 4.437 7.407 0 3.195-1.797 5.996-4.437 7.406-.492.258-1.102.07-1.36-.422-.257-.492-.07-1.102.422-1.359 2.012-1.075 3.375-3.176 3.375-5.625 0-2.446-1.371-4.551-3.375-5.625-.441-.204-.676-.692-.551-1.165.122-.468.567-.785 1.051-.742h.094z"></path></svg><div class="business-card-number"><div class="business-section">hgez6</div><div class="business-section">666666</div></div><div class="business-end"><span class="business-end-text">created:</span><span class="business-end-date">14/10/2022</span></div><div class="business-card-holder">Big Bird Luck Card</div><div class="business-master"><div class="business-circle business-master-red"></div><div class="business-circle business-master-yellow"></div></div></div><div class="business-back"><div class="business-strip-black"></div><div class="business-ccv"><label>鸟哥幸运卡</label><div><a target="_blank" rel="noopener" href="https://hgez6.top/">https://hgez6.top/</a></div></div><div class="business-terms"><p>Be happy .</p><p>This is luck card,wish you a nice day .</p></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload='this.media="all"'><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic1.zhimg.com/50/v2-37fc665d11aad76118a540e75498b5be_720w.webp?source=1940ef5c" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/50/v2-37fc665d11aad76118a540e75498b5be_720w.webp?source=1940ef5c" alt="Win!"></a><div class="post-qr-code-desc">Win!</div></li><li class="reward-item"><a href="https://pica.zhimg.com/50/v2-4a96a9401ec6f93f232a7bc5a72a072d_720w.webp?source=1940ef5c" target="_blank"><img class="post-qr-code-img" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pica.zhimg.com/50/v2-4a96a9401ec6f93f232a7bc5a72a072d_720w.webp?source=1940ef5c" alt="666~"></a><div class="post-qr-code-desc">666~</div></li></ul></div></button></div><audio id="coinAudio" src="https://yun.hgez6.top/api/raw/?path=/picbed/audio/aowu.m4a"></audio><script defer src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/83d847fd/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://yun.hgez6.top/api/raw/?path=/picbed/20.jpg" onerror='onerror=null,src="/assets/r2.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习白板推导</div></div></a></div><div class="next-post pull-right"><a href="/posts/d5bdee9a/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://yun.hgez6.top/api/raw/?path=/picbed/21.jpg" onerror='onerror=null,src="/assets/r2.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">甲磺酸73浓度siler分析</div></div></a></div></nav><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:700">目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83"><span class="toc-text">安装环境</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%8E%AF%E5%A2%83"><span class="toc-text">使用环境</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B7%A7%E7%94%A8dir-help-pyc%E7%BB%88%E7%AB%AF"><span class="toc-text">巧用dir,help,pyc终端</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#pyc%E5%88%87%E6%8D%A2%E8%A7%A3%E9%87%8A%E5%99%A8"><span class="toc-text">pyc切换解释器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%88%9D%E8%AE%A4%E8%AF%86"><span class="toc-text">PyTorch加载数据初认识</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p9"><span class="toc-text">P9</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p10"><span class="toc-text">P10</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84transforms"><span class="toc-text">常见的Transforms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E4%B8%AD-call-%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-text">类中__call__的用法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pyc%E6%9F%A5%E7%9C%8B%E7%BB%93%E6%9E%84"><span class="toc-text">pyc查看结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#randomcrop-%E9%9A%8F%E6%9C%BA%E8%A3%81%E5%89%AA%E4%BD%86%E4%B9%9F%E8%83%BD%E6%8C%87%E5%AE%9A%E5%8F%82%E6%95%B0"><span class="toc-text">RandomCrop 随机裁剪但也能指定参数</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#p15"><span class="toc-text">P15</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p16-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%AA%A8%E6%9E%B6-nn-module%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">P16 神经网络的基本骨架-nn.Module的使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p16"><span class="toc-text">P16</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p18"><span class="toc-text">P18</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p19"><span class="toc-text">P19</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p20"><span class="toc-text">P20</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p21"><span class="toc-text">P21</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p22"><span class="toc-text">P22</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p23"><span class="toc-text">P23</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p24"><span class="toc-text">P24</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p25"><span class="toc-text">P25</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p26"><span class="toc-text">P26</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p27"><span class="toc-text">P27</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#p28"><span class="toc-text">P28</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gpu%E8%AE%AD%E7%BB%83"><span class="toc-text">GPU训练</span></a></li></div></div></div></div></main><footer id="footer" style="background-color:transparent"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</div><div class="btn-xz-box"><a class="btn-xz" target="_blank" rel="noopener" href="https://stellarium.org/">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/posts/eec9786.html">魔改指南</a><a href="/box/nav/">网址导航</a></li><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/mypicture/">我的相册</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/aboutweb/">关于本站</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a target="_blank" rel="noopener" href="https://www.fomal.cc/" title="Fomalhaut🥝"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/60e5d4e39da7c077.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""></a></div></div></div></div><div class="copyright"><span><b>&copy;2021-2024</b></span><span><b>&nbsp;&nbsp;By 本大鸟就是玩✈️~</b></span></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hgez6.top/" style="margin-inline:5px" title="作者为hgez6"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/auther-hgez6-blueviolet.svg" alt=""></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/Theme-Butterfly-6513df.svg" alt=""></a><a class="github-badge" target="_blank" href="https://cn.bing.com/" style="margin-inline:5px" title="SEO 支持-Bing"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/SEO-bing-important.svg" alt=""></a><a class="github-badge" target="_blank" href="https://tongji.baidu.com/web/welcome/login" style="margin-inline:5px" title="统计支持-百度统计"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/analysis-Baidu-green.svg" alt=""></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/" style="margin-inline:5px" title="本站已加入萌ICP豪华套餐，萌ICP备20226066号"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/MICP.svg" alt=""></a><a class="github-badge" target="_blank" href="https://hgez6.top/aboutweb/" style="margin-inline:5px" title="使用模块化部署技术可整合多种前端框架"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" /img/badge/Technology-Modularization-ff96b4.svg" alt=""></a><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/Frame-Hexo-blue.svg" alt=""></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/badge/Page-Github-1586be.svg" alt=""></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight,500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i> <span>数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script async>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading()),setTimeout((function(){preloader.endLoading()}),5e3),document.getElementById("loading-box").addEventListener("click",(()=>{preloader.endLoading()}))</script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.2},options:{renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const a=!!e.type.match(/; *mode=display/),n=new t.options.MathItem(e.textContent,t.inputJax[0],a),s=document.createTextNode("");e.parentNode.replaceChild(s,e),n.start={node:s,delim:"",n:0},n.end={node:s,delim:"",n:0},t.math.push(n)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container:not([display])").forEach((t=>{const e=t.parentNode;"li"===e.nodeName.toLowerCase()?e.parentNode.classList.add("has-jax"):e.classList.add("has-jax")}))},"",!1]}}};const t=document.createElement("script");t.src="/pluginsSrc/mathjax/es5/tex-mml-chtml.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}</script><script>function addGitalkSource(){const t=document.createElement("link");t.rel="stylesheet",t.href="/pluginsSrc/gitalk/dist/gitalk.css",document.getElementsByTagName("head")[0].appendChild(t)}function loadGitalk(){function t(){new Gitalk(Object.assign({clientID:"df0b9c9de6d72825d4e1",clientSecret:"45cca8da59c6a1b17cb0fa0c02267b31a2b21df1",repo:"hgez6.github.io",owner:"hgez6",admin:["hgez6"],id:"fbaa7c6d961be51d56a1dca03dd24770",updateCountCallback:commentCount},null)).render("gitalk-container")}"function"==typeof Gitalk?t():(addGitalkSource(),getScript("/pluginsSrc/gitalk/dist/gitalk.min.js").then(t))}function commentCount(t){let e=document.querySelector("#post-meta .gitalk-comment-count");e&&(e.innerHTML=t)}{function loadOtherComment(){loadGitalk()}btf.loadComment(document.getElementById("gitalk-container"),loadGitalk)}</script></div><script src="/js/chajian/jquery.min.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script async src="/js/chajian/clipboard.min.js"></script><script defer src="/js/chajian/sweetalert2.all.js"></script><script async src="/js/chajian/pace.min.js"></script><script defer src="/js/chajian/winbox.bundle.min.js"></script><script async src="//at.alicdn.com/t/c/font_3586335_hsivh70x0fm.js"></script><script async src="//at.alicdn.com/t/c/font_3636804_gr02jmjr3y9.js"></script><script async src="//at.alicdn.com/t/c/font_3612150_kfv55xn3u2g.js"></script><script async src="/js/chajian/Gold-ingot.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><div class="aplayer no-destroy" data-id="2084738832" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listfolded="false" data-order="random" data-preload="none" data-autoplay="false" data-title="false" data-lrctype="1" muted></div><script async src="//at.alicdn.com/t/font_3148439_cfvkyebna8.js"></script><script async src="//at.alicdn.com/t/c/font_3148439_fsiy1xlmgzj.js"></script><script async src="//at.alicdn.com/t/font_2264842_3izu8i5eoc2.js"></script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script async data-pjax src="/js/diytianq.js"></script><script src="/js/chajian/echarts.min.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors=["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax","#bibi","body > title","#app","#tag-echarts","#posts-echart","#categories-echarts"];var pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(window.tocScrollFn&&window.removeEventListener("scroll",window.tocScrollFn),window.scrollCollect&&window.removeEventListener("scroll",scrollCollect),"object"==typeof preloader&&preloader.initLoading(),document.getElementById("rightside").style.cssText="opacity: ''; transform: ''",window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),"object"==typeof disqusjs&&disqusjs.destroy()})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)})),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof chatBtnFn&&chatBtnFn(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll(),"object"==typeof preloader&&preloader.endLoading()})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404.html")}))</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax>if(document.getElementById("recent-posts")&&"/"===location.pathname){var parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.hgez6.top/categories/嵌入式/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 小白瞎搞 (3)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.hgez6.top/categories/学习记录/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 持续学习！ (5)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.hgez6.top/categories/开源/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱‍👓 开放源码咯 (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.hgez6.top/categories/高等数学/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数学永远的神 (6)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://www.hgez6.top/categories/" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child)}</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(50% - 5px);background:#e9e9e9;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:var(--text-bg-hover)}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style><script data-pjax>function butterfly_swiper_injector_config(){var i=document.getElementById("recent-posts");console.log("已挂载butterfly_swiper"),i.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/65d9a22/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picsum.photos/id/635/300/300" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-21</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/65d9a22/&quot;);" href="javascript:void(0);" alt="">别忘了早上涂疤痕灵戴帽子！</a><div class="blog-slider__text">别忘了早上涂疤痕灵戴帽子</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/65d9a22/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/15a6564e/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picsum.photos/id/432/5000/3333" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-02</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/15a6564e/&quot;);" href="javascript:void(0);" alt="">TCP局域网内点对点控制方案</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/15a6564e/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/63350/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://picsum.photos/id/430/5000/3333" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-01</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/63350/&quot;);" href="javascript:void(0);" alt="">一种轻量化云端闭环设备间协同算法设计</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/63350/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>')}for(var elist="undefined".split(","),cpage=location.pathname,epage="/",flag=0,i=0;i<elist.length;i++)cpage.includes(elist[i])&&flag++;("all"===epage&&0==flag||epage===cpage)&&butterfly_swiper_injector_config()</script><script defer src="/js/chajian/swiper.min.js"></script><script defer data-pjax src="/js/chajian/swiper_init.js"></script><div class="js-pjax"><script async>for(var arr=document.getElementsByClassName("recent-post-item"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script><script async>for(var arr=document.getElementsByClassName("card-widget"),i=0;i<arr.length;i++)arr[i].classList.add("wow"),arr[i].classList.add("animate__zoomIn"),arr[i].setAttribute("data-wow-duration","2s"),arr[i].setAttribute("data-wow-delay","200ms"),arr[i].setAttribute("data-wow-offset","30"),arr[i].setAttribute("data-wow-iteration","1")</script></div><script defer src="/js/chajian/wow.min.js"></script><script defer src="/js/chajian/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>function gitcalendar_injector_config(){document.getElementById("recent-posts").insertAdjacentHTML("afterbegin",'<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 320px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>'),console.log("已挂载gitcalendar")}document.getElementById("recent-posts")&&"/"===location.pathname&&(gitcalendar_injector_config(),GitCalendarInit("https://gitapi.hgez6.top/api?hgez6",["#d9e0df","#c6e0dc","#a8dcd4","#9adcd2","#89ded1","#77e0d0","#5fdecb","#47dcc6","#39dcc3","#1fdabe","#00dab9"],"hgez6"))</script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script></body></html>